{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Hublog, import the following packages\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#for core, import the following packages\n",
    "import crc16\n",
    "\n",
    "#for raw, import the following packages\n",
    "import json\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "#for metadata, import the following packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "\n",
    "#for proximity, import the following packages\n",
    "import pandas as pd\n",
    "import json\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from core import mac_address_to_id\n",
    "def mac_address_to_id(mac):\n",
    "    \"\"\"Converts a MAC address to an id used by the badges for the proximity pings.\n",
    "    \"\"\"\n",
    "    # convert hex to bytes and reverse\n",
    "    macstr = mac.replace(':', '').decode('hex')[::-1]\n",
    "    crc = crc16.crc16xmodem(\"b\"+macstr,0xFFFF)\n",
    "    return crc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import from Raw\n",
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .raw import split_raw_data_by_day\n",
    "def split_raw_data_by_day(fileobject, target, kind, log_version=None):\n",
    "    \"\"\"Splits the data from a raw data file into a single file for each day.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : object, supporting tell, readline, seek, and iteration.\n",
    "        The raw data to be split, for instance, a file object open in read mode.\n",
    "\n",
    "    target : str\n",
    "        The directory into which the files will be written.  This directory must\n",
    "        already exist.\n",
    "\n",
    "    kind : str\n",
    "        The kind of data being extracted, either 'audio' or 'proximity'.\n",
    "\n",
    "    log_version : str\n",
    "        The log version, in case no metadata is present.\n",
    "    \"\"\"\n",
    "    # The days fileobjects\n",
    "    \n",
    "    # It's a mapping from iso dates (e.g. '2017-07-29') to fileobjects\n",
    "    days = {}\n",
    "    \n",
    "    # Extract log version from metadata, if present\n",
    "    log_version = extract_log_version(fileobject) or log_version\n",
    "\n",
    "    if log_version not in ('1.0', '2.0'):\n",
    "        raise Exception('file log version was not set and cannot be identified')\n",
    "\n",
    "    if log_version in ('1.0'):\n",
    "        raise Exception('file version '+str(log_version)+'is no longer supported')\n",
    "\n",
    "    # Read each line\n",
    "    for line in fileobject:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        # Keep only relevant data\n",
    "        if not data['type'] == kind + ' received':\n",
    "            continue\n",
    "\n",
    "        # Extract the day from the timestamp\n",
    "        day = datetime.date.fromtimestamp(data['data']['timestamp']).isoformat()\n",
    "\n",
    "        # If no fileobject exists for that day, create one\n",
    "        if day not in days:\n",
    "            days[day] = open(os.path.join(target, day), 'a')\n",
    "\n",
    "        # Write the data to the corresponding day file\n",
    "        json.dump(data, days[day])\n",
    "        days[day].write('\\n')\n",
    "    \n",
    "    # Free the memory\n",
    "    for f in days.values():\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import from Metadata\n",
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from .metadata import id_to_member_mapping\n",
    "def id_to_member_mapping(mapper, time_bins_size='1min', tz='US/Eastern', fill_gaps=True):\n",
    "    \"\"\"Creates a pd.Series mapping member numeric IDs to the string\n",
    "    member key associated with them. \n",
    "\n",
    "    If the 'mapper' provided is a DataFrame, assumes it's metadata and that ID's \n",
    "        do not change mapping throughout the project, and proceeds to create a\n",
    "        Series with only a member index.\n",
    "    If the 'mapper' provided is a file object, assumes the old version of id_map\n",
    "        and creates a Series with a datetime and member index.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file object\n",
    "        A file to read to determine the mapping.\n",
    "    \n",
    "    members_metadata : pd.DataFrame\n",
    "        Metadata dataframe, as downloaded from the server, to map IDs to keys.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series : \n",
    "        The ID to member key mapping.\n",
    "    \n",
    "    \"\"\"\n",
    "    if isinstance(mapper, io.BufferedIOBase) | isinstance(mapper, io.IOBase):\n",
    "        idmap = legacy_id_to_member_mapping(mapper, time_bins_size=time_bins_size, tz=tz, fill_gaps=fill_gaps)\n",
    "        print(type(mapper))\n",
    "        return idmap\n",
    "    elif isinstance(mapper, pd.DataFrame):\n",
    "        idmap = {row.member_id: row.member for row in mapper.itertuples()}\n",
    "        return pd.DataFrame.from_dict(idmap, orient='index')[0].rename('member')\n",
    "    else:\n",
    "        raise ValueError(\"You must provide either a fileobject or metadata dataframe as the mapper.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .metadata import legacy_id_to_member_mapping\n",
    "def legacy_id_to_member_mapping(fileobject, time_bins_size='1min', tz='US/Eastern', fill_gaps=True):\n",
    "    \"\"\"Creates a mapping from badge id to member, for each time bin, from proximity data file.\n",
    "    Depending on the version of the logfile (and it's content), it will either use the member_id\n",
    "    field to generate the mapping (newer version), or calculate an ID form the MAC address (this\n",
    "    was the default behavior of the older version of the hubs and badges)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The proximity data, as an iterable of JSON strings.\n",
    "    \n",
    "    time_bins_size : str\n",
    "        The size of the time bins used for resampling.  Defaults to '1min'.\n",
    "    \n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    fill_gaps : boolean\n",
    "        If True, the code will ensure that a value exists for every time by by filling the gaps\n",
    "        with the last seen value\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        A mapping from badge id to member, indexed by datetime and id.\n",
    "    \"\"\"\n",
    "    \n",
    "    def readfile(fileobject):\n",
    "        no_id_warning = False\n",
    "        for line in fileobject:\n",
    "            data = json.loads(line)['data']\n",
    "            member_id = None\n",
    "            if 'member_id' in data:\n",
    "                member_id = data['member_id']\n",
    "            else:\n",
    "                member_id = mac_address_to_id(data['badge_address'])\n",
    "                if not no_id_warning:\n",
    "                    print(\"Warning - no id provided in data. Calculating id from MAC address\")\n",
    "                no_id_warning = True\n",
    "\n",
    "            yield (data['timestamp'],\n",
    "                   member_id,\n",
    "                   str(data['member']))\n",
    "    \n",
    "    df = pd.DataFrame(readfile(fileobject), columns=['timestamp', 'id', 'member'])\n",
    "    # Convert the timestamp to a datetime, localized in UTC\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True) \\\n",
    "            .dt.tz_convert(tz)\n",
    "    del df['timestamp']\n",
    "\n",
    "    # Group by id and resample\n",
    "    df = df.groupby([\n",
    "        pd.Grouper(freq = time_bins_size, key='datetime'),\n",
    "        'id'\n",
    "    ]).first()\n",
    "\n",
    "    # Extract series\n",
    "    s = df.sort_index()['member']\n",
    "\n",
    "    # Fill in gaps, if requested to do so\n",
    "    if fill_gaps:\n",
    "        s = _id_to_member_mapping_fill_gaps(s, time_bins_size=time_bins_size)\n",
    "\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .metadata import voltages\n",
    "def voltages(fileobject, time_bins_size='1min', tz='US/Eastern', skip_errors=False):\n",
    "    \"\"\"Creates a DataFrame of voltages, for each member and time bin.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The proximity data, as an iterable of JSON strings.\n",
    "    \n",
    "    time_bins_size : str\n",
    "        The size of the time bins used for resampling.  Defaults to '1min'.\n",
    "    \n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    skip_errors : boolean\n",
    "        If set to True, skip errors in the data file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        Voltages, indexed by datetime and member.\n",
    "    \"\"\"\n",
    "    \n",
    "    def readfile(fileobject, skip_errors):\n",
    "        i = 0\n",
    "        for line in fileobject:\n",
    "            i = i + 1\n",
    "            try:\n",
    "                data = json.loads(line)['data']\n",
    "\n",
    "                yield (data['timestamp'],\n",
    "                       str(data['member']),\n",
    "                       float(data['voltage']))\n",
    "            except:\n",
    "                print(\"Error in line#:\", i, line)\n",
    "                if skip_errors:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    df = pd.DataFrame(readfile(fileobject, skip_errors), columns=['timestamp', 'member', 'voltage'])\n",
    "\n",
    "    # Convert the timestamp to a datetime, localized in UTC\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True) \\\n",
    "                       .dt.tz_convert(tz)\n",
    "    del df['timestamp']\n",
    "\n",
    "    # Group by id and resample\n",
    "    df = df.groupby([\n",
    "        pd.TimeGrouper(time_bins_size, key='datetime'),\n",
    "        'member'\n",
    "    ]).mean()\n",
    "    \n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    return df['voltage']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .metadata import sample_counts\n",
    "def sample_counts(fileobject, tz='US/Eastern', keep_type=False, skip_errors=False):\n",
    "    \"\"\"Creates a DataFrame of sample counts, for each member and raw record\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The proximity or audio data, as an iterable of JSON strings.\n",
    "\n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    keep_type : boolean\n",
    "        If set to True, the type of the record will be returned as well\n",
    "\n",
    "    skip_errors : boolean\n",
    "        If set to True, skip errors in the data file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        Counts, indexed by datetime, type and member.\n",
    "    \"\"\"\n",
    "\n",
    "    def readfile(fileobject, skip_errors=False):\n",
    "        i = 0\n",
    "        for line in fileobject:\n",
    "            i = i + 1\n",
    "            try:\n",
    "                raw_data = json.loads(line)\n",
    "                data = raw_data['data']\n",
    "                type = raw_data['type']\n",
    "\n",
    "                if type == 'proximity received':\n",
    "                    cnt = len(data['rssi_distances'])\n",
    "                elif type == 'audio received':\n",
    "                    cnt = len(data['samples'])\n",
    "                else:\n",
    "                    cnt = -1\n",
    "\n",
    "                yield (data['timestamp'],\n",
    "                       str(type),\n",
    "                       str(data['member']),\n",
    "                       int(cnt))\n",
    "            except:\n",
    "                print(\"Error in line#:\", i, line)\n",
    "                if skip_errors:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "\n",
    "    df = pd.DataFrame(readfile(fileobject, skip_errors), columns=['timestamp' ,'type', 'member',\n",
    "                                                     'cnt'])\n",
    "\n",
    "    # Convert the timestamp to a datetime, localized in UTC\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True) \\\n",
    "        .dt.tz_convert(tz)\n",
    "    del df['timestamp']\n",
    "\n",
    "    if keep_type:\n",
    "        df.set_index(['datetime','type','member'],inplace=True)\n",
    "    else:\n",
    "        del df['type']\n",
    "        df.set_index(['datetime', 'member'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _id_to_member_mapping_fill_gaps(idmap, time_bins_size='1min'):\n",
    "    \"\"\" Fill gaps in a idmap\n",
    "    Parameters\n",
    "    ----------\n",
    "    idmap : id mapping object\n",
    "\n",
    "    time_bins_size : str\n",
    "        The size of the time bins used for resampling.  Defaults to '1min'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        idmap, after filling gaps.\n",
    "    \"\"\"\n",
    "    df = idmap.to_frame().reset_index()\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    #df.index = pd.to_datetime(df.index,unit='s')\n",
    "    s = df.groupby(['id'])['member'].resample(time_bins_size).fillna(method='ffill')\n",
    "    s = s.reorder_levels((1,0)).sort_index()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import from Proximity\n",
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .proximity import member_to_badge_proximity\n",
    "def member_to_badge_proximity(fileobject, time_bins_size='1min', tz='US/Eastern'):\n",
    "    \"\"\"Creates a member-to-badge proximity DataFrame from a proximity data file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The proximity data, as an iterable of JSON strings.\n",
    "    \n",
    "    time_bins_size : str\n",
    "        The size of the time bins used for resampling.  Defaults to '1min'.\n",
    "    \n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The member-to-badge proximity data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def readfile(fileobject):\n",
    "        for line in fileobject:\n",
    "            data = json.loads(line)['data']\n",
    "\n",
    "            for (observed_id, distance) in data['rssi_distances'].items():\n",
    "                yield (\n",
    "                    data['timestamp'],\n",
    "                    str(data['member']),\n",
    "                    int(observed_id),\n",
    "                    float(distance['rssi']),\n",
    "                    float(distance['count']),\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "            readfile(fileobject),\n",
    "            columns=('timestamp', 'member', 'observed_id', 'rssi', 'count')\n",
    "    )\n",
    "\n",
    "    # Convert timestamp to datetime for convenience, and localize to UTC\n",
    "    df['datetime'] = pd.to_datetime(df['timestamp'], unit='s', utc=True) \\\n",
    "        .dt.tz_convert(tz)\n",
    "    del df['timestamp']\n",
    "\n",
    "    # Group per time bins, member and observed_id,\n",
    "    # and take the first value, arbitrarily\n",
    "    df = df.groupby([\n",
    "        pd.Grouper(freq = time_bins_size, key='datetime'),\n",
    "        'member',\n",
    "        'observed_id'\n",
    "    ]).first()\n",
    "\n",
    "    # Sort the data\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .proximity import member_to_member_proximity\n",
    "def member_to_member_proximity(m2badge, id2m):\n",
    "    \"\"\"Creates a member-to-member proximity DataFrame from member-to-badge proximity data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    m2badge : pd.DataFrame\n",
    "        The member-to-badge proximity data, as returned by `member_to_badge_proximity`.\n",
    "\n",
    "    id2m : pd.Series\n",
    "        The badge IDs used by each member, indexed by datetime and badge id, as returned by\n",
    "        `id_to_member_mapping`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The member-to-member proximity data.\n",
    "    \"\"\"\n",
    "\n",
    "    df = m2badge.copy().reset_index()\n",
    "    \n",
    "    # Join the member names using their badge ids\n",
    "    # If id2m index is a MultiIndex, assume it is a time series and use legacy method\n",
    "    if type(id2m.index) == pd.MultiIndex:\n",
    "        df = df.join(id2m, on=['datetime', 'observed_id'], lsuffix='1', rsuffix='2')\n",
    "    # Otherwise, assume it is not time-based, and join without datetime\n",
    "    else:\n",
    "        df = df.join(id2m, on=['observed_id'], lsuffix='1', rsuffix='2')\n",
    "    \n",
    "    # Filter out the beacons (i.e. those ids that did not have a mapping)\n",
    "    df.dropna(axis=0, subset=['member2'], inplace=True)\n",
    "\n",
    "    # Reset the members type to their original type\n",
    "    # This is done because pandas likes to convert ints to floats when there are\n",
    "    # missing values\n",
    "    df['member2'] = df['member2'].astype(id2m.dtype)\n",
    "        \n",
    "    # Set the index and sort it\n",
    "    df.set_index(['datetime', 'member1', 'member2'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Remove duplicate indexes, keeping the first (arbitrarily)\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "    # If the dataframe is empty after the join, we can (and should) stop\n",
    "    # here\n",
    "    if len(df) == 0:\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    # Reorder the index such that 'member1' is always lexicographically smaller than 'member2'\n",
    "    df.index = df.index.map(lambda ix: (ix[0], min(ix[1], ix[2]), max(ix[1], ix[2])))\n",
    "    df.index.names = ['datetime', 'member1', 'member2']\n",
    "\n",
    "    # For cases where we had proximity data coming from both sides,\n",
    "    # we calculate two types of rssi:\n",
    "    # * weighted_mean - take the average RSSI weighted by the counts, and the sum of the counts\n",
    "    # * max - take the max value\n",
    "    df['rssi_weighted'] = df['count'] * df['rssi']\n",
    "    agg_f = collections.OrderedDict([('rssi', ['max']), ('rssi_weighted', ['sum']), ('count', ['sum'])])\n",
    "\n",
    "    df = df.groupby(level=df.index.names).agg(agg_f)\n",
    "    df['rssi_weighted'] /= df['count']\n",
    "\n",
    "    # rename columns\n",
    "    df.columns = ['rssi_max', 'rssi_weighted_mean', 'count_sum']\n",
    "    df['rssi'] = df['rssi_weighted_mean']  # for backward compatibility\n",
    "\n",
    "    # Select only the fields 'rssi' and 'count'\n",
    "    print(df)\n",
    "    return df[['rssi', 'rssi_max', 'rssi_weighted_mean', 'count_sum']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .proximity import member_to_beacon_proximity\n",
    "def member_to_beacon_proximity(m2badge, id2b):\n",
    "    \"\"\"Creates a member-to-beacon proximity DataFrame from member-to-badge proximity data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m2badge : pd.DataFrame\n",
    "        The member-to-badge proximity data, as returned by `member_to_badge_proximity`.\n",
    "    \n",
    "    id2b : pd.Series\n",
    "        A mapping from badge ID to beacon name.  Index must be ID, and series name must be 'beacon'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The member-to-beacon proximity data.\n",
    "    \"\"\"\n",
    "    \n",
    "    df = m2badge.copy().reset_index()\n",
    "\n",
    "    # Join the beacon names using their badge ids\n",
    "    df = df.join(id2b, on='observed_id') \n",
    "\n",
    "    # Filter out the members (i.e. those ids that did not have a mapping)\n",
    "    df.dropna(axis=0, subset=['beacon'], inplace=True)\n",
    "\n",
    "    # Reset the beacons type to their original type\n",
    "    # This is done because pandas likes to convert ints to floats when there are\n",
    "    # missing values\n",
    "    df['beacon'] = df['beacon'].astype(id2b.dtype)\n",
    "\n",
    "    # Set the index and sort it\n",
    "    df.set_index(['datetime', 'member', 'beacon'], inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Remove duplicate indexes, keeping the first (arbitrarily)\n",
    "    df = df[~df.index.duplicated(keep='first')]\n",
    "\n",
    "    return df[['rssi']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .proximity import member_to_beacon_proximity_smooth\n",
    "def member_to_beacon_proximity_smooth(m2b, window_size = '5min',\n",
    "                                      min_samples = 1):\n",
    "    \"\"\" Smooths the given object using 1-D median filter\n",
    "    Parameters\n",
    "    ----------\n",
    "    m2b : Member to beacon object\n",
    "\n",
    "    window_size : str\n",
    "        The size of the window used for smoothing.  Defaults to '5min'.\n",
    "\n",
    "    min_samples : int\n",
    "        Minimum number of samples required for smoothing\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The member-to-beacon proximity data, after smoothing.\n",
    "    \"\"\"\n",
    "    df = m2b.copy().reset_index()\n",
    "    df = df.sort_values(by=['member', 'beacon', 'datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    df2 = df.groupby(['member', 'beacon'])[['rssi']] \\\n",
    "        .rolling(window=window_size, min_periods=min_samples) \\\n",
    "        .median()\n",
    "\n",
    "    # For std, we put-1 when std was NaN. This handles the case\n",
    "    # when there was only one record. If there were no records (\n",
    "    # median was not calculated because of min_samples), the record\n",
    "    # will be dropped because of the NaN in 'rssi'\n",
    "    df2['rssi_std']\\\n",
    "        = df.groupby(['member', 'beacon'])[['rssi']] \\\n",
    "        .rolling(window=window_size, min_periods=min_samples) \\\n",
    "        .std().fillna(-1)\n",
    "\n",
    "    # number of records used for calculating the median\n",
    "    df2['rssi_smooth_window_count']\\\n",
    "        = df.groupby(['member', 'beacon'])[['rssi']] \\\n",
    "        .rolling(window=window_size, min_periods=min_samples) \\\n",
    "        .count()\n",
    "\n",
    "    df2 = df2.reorder_levels(['datetime', 'member', 'beacon'], axis=0)\\\n",
    "        .dropna().sort_index()\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .proximity import member_to_beacon_proximity_fill_gaps\n",
    "def member_to_beacon_proximity_fill_gaps(m2b, time_bins_size='1min',\n",
    "                                        max_gap_size = 2):\n",
    "    \"\"\" Fill gaps in a given member to beacon object\n",
    "    Parameters\n",
    "    ----------\n",
    "    m2b : Member to beacon object\n",
    "\n",
    "    time_bins_size : str\n",
    "        The size of the time bins used for resampling.  Defaults to '1min'.\n",
    "\n",
    "    max_gap_size : int\n",
    "         this is the maximum number of consecutive NaN values to forward/backward fill\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The member-to-beacon proximity data, after filling gaps.\n",
    "    \"\"\"\n",
    "\n",
    "    df = m2b.copy().reset_index()\n",
    "    df = df.sort_values(by=['member', 'beacon', 'datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "\n",
    "    df = df.groupby(['member', 'beacon']) \\\n",
    "        [['rssi', 'rssi_std','rssi_smooth_window_count']] \\\n",
    "        .resample(time_bins_size) \\\n",
    "        .fillna(method='ffill', limit=max_gap_size)\n",
    "\n",
    "    df = df.reorder_levels(['datetime', 'member', 'beacon'], axis=0)\\\n",
    "        .dropna().sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .hublog import hublog_scans\n",
    "def hublog_scans(fileobject, log_tz, tz='US/Eastern'):\n",
    "    \"\"\"Creates a DataFrame of hub scans.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The raw log file from a hub.\n",
    "\n",
    "    log_tz : str\n",
    "        The time zone used in the logfile itself\n",
    "\n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        A scan record with mac, rssi, and device status (if available)\n",
    "    \"\"\"\n",
    "\n",
    "    def readfile(fileobject):\n",
    "        for line in fileobject:\n",
    "            line_num = line_num + 1\n",
    "            data = _hublog_read_scan_line(line)\n",
    "            if data:\n",
    "                yield (data['datetime'],\n",
    "                       str(data['mac']),\n",
    "                       float(data['rssi']),\n",
    "                       data['voltage'],\n",
    "                       data['badge_id'],\n",
    "                       data['project_id'],\n",
    "                       data['sync_status'],\n",
    "                       data['audio_status'],\n",
    "                       data['proximity_status'],\n",
    "                       )\n",
    "            else:\n",
    "                continue  # skip unneeded lines\n",
    "\n",
    "    df = pd.DataFrame(readfile(fileobject), columns=['datetime', 'mac', 'rssi', 'voltage', 'badge_id', \\\n",
    "                                                     'project_id', 'sync_status', 'audio_status', \\\n",
    "                                                     'proximity_status'])\n",
    "\n",
    "    # Localized record date\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True) \\\n",
    "        .dt.tz_localize(log_tz).dt.tz_convert(tz)\n",
    "\n",
    "    # Sort\n",
    "    df = df.set_index('datetime')\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .hublog import hublog_resets\n",
    "def hublog_resets(fileobject, log_tz, tz='US/Eastern'):\n",
    "    \"\"\"Creates a DataFrame of reset events - when badge were previously not synced and\n",
    "        the hub sent a new date\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The raw log file from a hub.\n",
    "\n",
    "    log_tz : str\n",
    "        The time zone used in the logfile itself\n",
    "\n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        A record with mac and timestamp\n",
    "    \"\"\"\n",
    "    def readfile(fileobject):\n",
    "        for line in fileobject:\n",
    "            data = _hublog_read_reset_line(line)\n",
    "            if data:\n",
    "                yield (data['datetime'],\n",
    "                       str(data['mac']),\n",
    "                       )\n",
    "            else:\n",
    "                continue  # skip unneeded lines\n",
    "\n",
    "    df = pd.DataFrame(readfile(fileobject), columns=['datetime', 'mac'])\n",
    "\n",
    "    # Localized record date\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True) \\\n",
    "        .dt.tz_localize(log_tz).dt.tz_convert(tz)\n",
    "\n",
    "    # Sort\n",
    "    df = df.set_index('datetime')\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from .hublog import hublog_clock_syncs\n",
    "def hublog_clock_syncs(fileobject, log_tz, tz='US/Eastern'):\n",
    "    \"\"\"Creates a DataFrame of sync events - when badge were previously not synced and\n",
    "        the hub sent a new date\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fileobject : file or iterable list of str\n",
    "        The raw log file from a hub.\n",
    "\n",
    "    log_tz : str\n",
    "        The time zone used in the logfile itself\n",
    "\n",
    "    tz : str\n",
    "        The time zone used for localization of dates.  Defaults to 'US/Eastern'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series :\n",
    "        A record with mac and timestamps\n",
    "    \"\"\"\n",
    "\n",
    "    def readfile(fileobject):\n",
    "        for line in fileobject:\n",
    "            data = _hublog_read_clock_sync_line(line)\n",
    "            if data:\n",
    "                yield (data['datetime'],\n",
    "                       str(data['mac']),\n",
    "                       str(data['badge_timestamp']),\n",
    "                       )\n",
    "            else:\n",
    "                continue  # skip unneeded lines\n",
    "\n",
    "    df = pd.DataFrame(readfile(fileobject), columns=['datetime', 'mac', 'badge_timestamp'])\n",
    "\n",
    "    # Localized record date\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'], utc=True) \\\n",
    "        .dt.tz_localize(log_tz).dt.tz_convert(tz)\n",
    "\n",
    "    # Convert the badge timestamp to a datetime, localized in UTC\n",
    "    df['badge_datetime'] = pd.to_datetime(df['badge_timestamp'], unit='s', utc=True) \\\n",
    "        .dt.tz_localize('UTC').dt.tz_convert(tz)\n",
    "    del df['badge_timestamp']\n",
    "\n",
    "    # Sort\n",
    "    df = df.set_index('datetime')\n",
    "    df.sort_index(inplace=True)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
